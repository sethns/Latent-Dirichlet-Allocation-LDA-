{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# import vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# import LDA from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the documents into a list:\n",
    "\n",
    "corpus = [D1, D2, D3, D4, D5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6177,
     "status": "ok",
     "timestamp": 1603266387620,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "fNATC-NcaVrt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the complete corpus as below:\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n",
    "\n",
    "Steps to preprocess text data:\n",
    "\n",
    "1. Convert the text into lowercase\n",
    "2. Split text into words\n",
    "3. Remove the stop loss words\n",
    "3. Remove the Punctuation, any symbols and special characters\n",
    "4. Normalize the word (I'll be using Lemmatization for normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Preprocessing on the Corpus\n",
    "\n",
    "# stop loss words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekend'],\n",
       " ['went',\n",
       "  'shopping',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beating',\n",
       "  'india',\n",
       "  'eight',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['don’t',\n",
       "  'watch',\n",
       "  'cricket',\n",
       "  'netflix',\n",
       "  'amazon',\n",
       "  'prime',\n",
       "  'good',\n",
       "  'movie',\n",
       "  'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'however',\n",
       "  'time',\n",
       "  'would',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'it’s',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'dr',\n",
       "  'joe',\n",
       "  'dispenza’s',\n",
       "  'book',\n",
       "  'work',\n",
       "  'gamechanger',\n",
       "  'book',\n",
       "  'helped',\n",
       "  'learn',\n",
       "  'much',\n",
       "  'thought',\n",
       "  'impact',\n",
       "  'biology',\n",
       "  'rewire',\n",
       "  'brain']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Text into Numerical Representation\n",
    "\n",
    "Converting the clean preprocessed corpus to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text into numerical representation\n",
    "tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "\n",
    "# Converting text into numerical representation\n",
    "cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1795,
     "status": "ok",
     "timestamp": 1603266401016,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "vgjAOvoZaVr0"
   },
   "outputs": [],
   "source": [
    "# Array from TF-IDF Vectorizer \n",
    "tf_idf_arr = tf_idf_vectorizer.fit_transform(clean_corpus)\n",
    "\n",
    "# Array from Count Vectorizer \n",
    "cv_arr = cv_vectorizer.fit_transform(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x52 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 58 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our converted text to numerical representation from the Tf-IDF vectorizer\n",
    "\n",
    "tf_idf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x52 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 58 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our converted text to numerical representation from the Count vectorizer\n",
    "cv_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 7256,
     "status": "ok",
     "timestamp": 1603266428543,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "0ygFEWCMaVr6",
    "outputId": "599a8c11-aae8-418d-efb6-28ae1080a713"
   },
   "source": [
    "The corpus has 52 columns and 5 rows corresponding to our document and 58 represents the unique Vocabulary present in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1603266495547,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "PmdKIbSEaVr_",
    "outputId": "f93e404f-c5db-41d6-9979-c008d68fe5fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon',\n",
       " 'beating',\n",
       " 'biology',\n",
       " 'blueberry',\n",
       " 'book',\n",
       " 'brain',\n",
       " 'championship',\n",
       " 'chill',\n",
       " 'cricket',\n",
       " 'dispenza’s',\n",
       " 'don’t',\n",
       " 'dr',\n",
       " 'eight',\n",
       " 'gamechanger',\n",
       " 'good',\n",
       " 'helped',\n",
       " 'however',\n",
       " 'impact',\n",
       " 'india',\n",
       " 'it’s',\n",
       " 'joe',\n",
       " 'learn',\n",
       " 'like',\n",
       " 'long',\n",
       " 'milkshake',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'netflix',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'paint',\n",
       " 'prime',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'rewire',\n",
       " 'shopping',\n",
       " 'southampton',\n",
       " 'test',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'try',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'weekend',\n",
       " 'went',\n",
       " 'wicket',\n",
       " 'work',\n",
       " 'world',\n",
       " 'would',\n",
       " 'yesterday',\n",
       " 'zealand']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vocabulary array which will represent all the corpus \n",
    "vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
    "\n",
    "# get the vocb list\n",
    "vocab_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1603266495547,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "PmdKIbSEaVr_",
    "outputId": "f93e404f-c5db-41d6-9979-c008d68fe5fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon',\n",
       " 'beating',\n",
       " 'biology',\n",
       " 'blueberry',\n",
       " 'book',\n",
       " 'brain',\n",
       " 'championship',\n",
       " 'chill',\n",
       " 'cricket',\n",
       " 'dispenza’s',\n",
       " 'don’t',\n",
       " 'dr',\n",
       " 'eight',\n",
       " 'gamechanger',\n",
       " 'good',\n",
       " 'helped',\n",
       " 'however',\n",
       " 'impact',\n",
       " 'india',\n",
       " 'it’s',\n",
       " 'joe',\n",
       " 'learn',\n",
       " 'like',\n",
       " 'long',\n",
       " 'milkshake',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'netflix',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'paint',\n",
       " 'prime',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'rewire',\n",
       " 'shopping',\n",
       " 'southampton',\n",
       " 'test',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'try',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'weekend',\n",
       " 'went',\n",
       " 'wicket',\n",
       " 'work',\n",
       " 'world',\n",
       " 'would',\n",
       " 'yesterday',\n",
       " 'zealand']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating vocabulary array which will represent all the corpus \n",
    "vocab_cv = cv_vectorizer.get_feature_names()\n",
    "\n",
    "# get the vocb list\n",
    "vocab_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(vocab_tf_idf))\n",
    "display(len(vocab_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementation of LDA\n",
    "\n",
    "To implement LDA, pass the corpus: document-term matrix to the model. We had above obtained the unique words of vocabulary using both TF-IDF and Count Vectorizer. We can continue with either as have the same unique words in both the obtained vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1338,
     "status": "ok",
     "timestamp": 1603266590630,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "9X5BJB_aaVsE"
   },
   "outputs": [],
   "source": [
    " # Implementation of LDA:\n",
    "    \n",
    "# Create object for the LDA class \n",
    "# Inside this class LDA: define the components:\n",
    "lda_model = LatentDirichletAllocation(n_components = 6, max_iter = 20, random_state = 20)\n",
    "\n",
    "# fit transform on model on our count_vectorizer : running this will return our topics \n",
    "X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "\n",
    "# .components_ gives us our topic distribution \n",
    "topic_words = lda_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Retrieve the Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 1463,
     "status": "ok",
     "timestamp": 1603266703310,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "H9zZ2_77aVsJ",
    "outputId": "8a02323a-4346-46f2-ad1d-86295c349a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 ['movie' 'good' 'watch' 'book']\n",
      "Topic 2 ['zealand' 'test' 'beating' 'world']\n",
      "Topic 3 ['weekend' 'want' 'watch' 'movie']\n",
      "Topic 4 ['watch' 'amazon' 'cricket' 'don’t']\n",
      "Topic 5 ['movie' 'good' 'watch' 'book']\n",
      "Topic 6 ['however' 'chill' 'would' 'it’s']\n"
     ]
    }
   ],
   "source": [
    "#  Define the number of Words that we want to print in every topic : n_top_words\n",
    "n_top_words = 5\n",
    "\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    \n",
    "    # np.argsort to sorting an array or a list or the matrix acc to their values\n",
    "    sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "    # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
    "    topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    \n",
    "    # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
    "    # obtaining topics + words\n",
    "    # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
    "    topic_words = topic_words[:-n_top_words:-1]\n",
    "    print (\"Topic\", str(i+1), topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the words per topic. The result is not so accurate as the data was less. More data will give more accurate result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Annotating the topics the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 2712,
     "status": "ok",
     "timestamp": 1603266754892,
     "user": {
      "displayName": "Aishwarya Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgeJwfn4BdBDCAplWi_kdtB9FRssOpXO7T_aMgg=s64",
      "userId": "01105858832371513140"
     },
     "user_tz": -330
    },
    "id": "-s31RUYKaVsQ",
    "outputId": "75c04a6f-835f-46dc-d4d4-3a5c0e75996c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1  -- Topic: 2\n",
      "Document 2  -- Topic: 1\n",
      "Document 3  -- Topic: 3\n",
      "Document 4  -- Topic: 5\n",
      "Document 5  -- Topic: 2\n"
     ]
    }
   ],
   "source": [
    "# To view what topics are assigned to the douments:\n",
    "\n",
    "doc_topic = lda_model.transform(tf_idf_arr)  \n",
    "\n",
    "# iterating over ever value till the end value\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    \n",
    "    # argmax() gives maximum index value\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    \n",
    "    # document is n+1  \n",
    "    print (\"Document\", n+1, \" -- Topic:\" ,topic_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final output which gives us the topic along with the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3. Interpreting Patterns from Text - Topic Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
